{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Layer\n",
    "\n",
    "This notebook walks through how to build a sequential layer type, allowing you to chain an arbitrary number of layers of the same type together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/ubuntu/git/fastai_docs/dev_swift/FastaiNotebook_00_load_data\")\n",
      "\t\tFastaiNotebook_00_load_data\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmpcrltwead\n",
      "/home/ubuntu/swift/usr/bin/swift-build: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/lib/swift/linux/libFoundation.so)\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Completed resolution in 1.26s\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "/home/ubuntu/swift/usr/bin/swiftc: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swiftc)\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "/home/ubuntu/swift/usr/bin/swiftc: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swiftc: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'FastaiNotebook_00_load_data' (1 sources)\n",
      "/home/ubuntu/swift/usr/bin/swiftc: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "/home/ubuntu/swift/usr/bin/swiftc: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift)\n",
      "\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "/home/ubuntu/swift/usr/bin/swiftc: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/ubuntu/swift/usr/bin/swift-autolink-extract: /home/ubuntu/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/ubuntu/swift/usr/bin/swift-autolink-extract)\n",
      "\n",
      "Initializing Swift...\n",
      "Loading library...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_00_load_data\")' FastaiNotebook_00_load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_00_load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "\n",
    "/// Define a new Differentiable data type that will be the AllDifferentiableVariables, Cotangent-, and Tangent vectors\n",
    "/// for our sequential layer type.\n",
    "public struct DiffList<U: Differentiable & AdditiveArithmetic & Equatable & VectorNumeric>: KeyPathIterable {\n",
    "    public var u: [U] = []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension DiffList: Equatable {\n",
    "    public static func == (lhs: DiffList, rhs: DiffList) -> Bool {\n",
    "       if lhs.u.count != rhs.u.count { return false }\n",
    "       for i in 0..<lhs.u.count {\n",
    "          if lhs.u[i] != rhs.u[i] { return false }\n",
    "       }\n",
    "       return true\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "\n",
    "extension DiffList: AdditiveArithmetic {\n",
    "    public static var zero: DiffList {\n",
    "        get { return DiffList() }\n",
    "    }\n",
    "    \n",
    "    @differentiable(vjp: _vjpAdd(lhs:rhs:))\n",
    "    public static func + (lhs: DiffList, rhs: DiffList) -> DiffList {\n",
    "        precondition(lhs.u.count == 0 || rhs.u.count == 0 || lhs.u.count == rhs.u.count,\n",
    "                     \"DiffList size mis-match: lhs: \\(lhs.u.count), rhs: \\(rhs.u.count)\")\n",
    "        if lhs.u.count == 0 { return rhs }\n",
    "        if rhs.u.count == 0 { return lhs }\n",
    "        var output: [U] = []\n",
    "        for i in 0..<lhs.u.count { output.append(lhs.u[i] + rhs.u[i]) }\n",
    "        return DiffList(u: output)\n",
    "    }\n",
    "    \n",
    "    public static func _vjpAdd(lhs: DiffList, rhs: DiffList) -> (DiffList, (DiffList) -> (DiffList, DiffList)) {\n",
    "        return (lhs + rhs, { [lhsCount = lhs.u.count, rhsCount = rhs.u.count] v in\n",
    "            precondition(v.u.count == lhsCount || lhsCount == 0, \n",
    "                         \"DiffList gradient size mis-match: v: \\(v.u.count), lhs: \\(lhsCount)\")\n",
    "            precondition(v.u.count == rhsCount || rhsCount == 0,\n",
    "                         \"DiffList gradient size mis-match: v: \\(v.u.count), rhs: \\(rhsCount)\")\n",
    "            var lhsOutput: [U]  = []\n",
    "            var rhsOutput: [U]  = []\n",
    "            // Unbroadcasting\n",
    "            if lhsCount != 0 { lhsOutput = v.u }\n",
    "            if rhsCount != 0 { rhsOutput = v.u }\n",
    "            return (DiffList(u: lhsOutput), DiffList(u: rhsOutput))\n",
    "        })\n",
    "    }\n",
    "\n",
    "    @differentiable(vjp: _vjpSubtract(lhs:rhs:))\n",
    "    public static func - (lhs: DiffList, rhs: DiffList) -> DiffList {\n",
    "        precondition(lhs.u.count == 0 || rhs.u.count == 0 || lhs.u.count == rhs.u.count,\n",
    "                     \"DiffList size mis-match: lhs: \\(lhs.u.count), rhs: \\(rhs.u.count)\")\n",
    "        if lhs.u.count == 0 { return rhs }\n",
    "        if rhs.u.count == 0 { return lhs }\n",
    "        var output: [U] = []\n",
    "        for i in 0..<lhs.u.count { output.append(lhs.u[i] + rhs.u[i]) }\n",
    "        return DiffList(u: output)\n",
    "    }\n",
    "\n",
    "    public static func _vjpSubtract(lhs: DiffList, rhs: DiffList) -> (DiffList, (DiffList) -> (DiffList, DiffList)) {\n",
    "        return (lhs + rhs, { [lhsCount = lhs.u.count, rhsCount = rhs.u.count] v in\n",
    "            precondition(v.u.count == lhsCount || lhsCount == 0,\n",
    "                         \"DiffList gradient size mis-match: v: \\(v.u.count), lhs: \\(lhsCount)\")\n",
    "            precondition(v.u.count == rhsCount || rhsCount == 0,\n",
    "                         \"DiffList gradient size mis-match: v: \\(v.u.count), rhs: \\(rhsCount)\")\n",
    "\n",
    "            var lhsOutput: [U]  = []\n",
    "            var rhsOutput: [U]  = []\n",
    "            // Unbroadcasting\n",
    "            if lhsCount != 0 { lhsOutput = v.u }\n",
    "            if rhsCount != 0 { rhsOutput = v.u.map({ U.zero - $0 }) }\n",
    "            return (DiffList(u: lhsOutput), DiffList(u: rhsOutput))\n",
    "        })\n",
    "    }\n",
    "}\n",
    "\n",
    "extension DiffList: VectorNumeric {\n",
    "    public typealias Scalar = U.Scalar\n",
    "    \n",
    "    public static func * (lhs: Scalar, rhs: DiffList) -> DiffList {\n",
    "        return DiffList(u: rhs.u.map( { $0 * lhs } ))\n",
    "    }\n",
    "}\n",
    "\n",
    "extension DiffList: Differentiable {\n",
    "    public typealias TangentVector = DiffList\n",
    "    public typealias CotangentVector = DiffList\n",
    "    public typealias AllDifferentiableVariables = DiffList\n",
    "\n",
    "    public func tangentVector(from cotangent: CotangentVector) -> TangentVector {\n",
    "        return cotangent\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import TensorFlow  // Defines Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "\n",
    "/// A struct that contains a number of layers within it.\n",
    "public struct SequentialLayer<U: Layer>: KeyPathIterable where \n",
    "    U.Input == U.Output,\n",
    "    U.AllDifferentiableVariables: VectorNumeric,\n",
    "    U.AllDifferentiableVariables == U.CotangentVector {\n",
    "\n",
    "    public var layers: [U]\n",
    "\n",
    "    public init(layers: [U]) {\n",
    "        self.layers = layers\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "// Mark SequentialLayer as Differentiable\n",
    "extension SequentialLayer: Differentiable {\n",
    "    public typealias AllDifferentiableVariables = DiffList<U.AllDifferentiableVariables>\n",
    "    public typealias TangentVector = DiffList<U.TangentVector>\n",
    "    public typealias CotangentVector = DiffList<U.CotangentVector>\n",
    "\n",
    "    public func tangentVector(from cotangent: CotangentVector) -> TangentVector {\n",
    "        precondition(cotangent.u.count == layers.count, \"Differing # of layers: \\(cotangent.u.count) and \\(layers.count)\")\n",
    "        return DiffList(u: zip(layers, cotangent.u).map({ $0.0.tangentVector(from: $0.1) }))\n",
    "    }\n",
    "\n",
    "    public func moved(along direction: TangentVector) -> SequentialLayer {\n",
    "        precondition(direction.u.count == layers.count, \"Differing # of layers: \\(direction.u.count) and \\(layers.count)\")\n",
    "        return SequentialLayer(layers: zip(layers, direction.u).map({ $0.0.moved(along: $0.1) }))\n",
    "    }\n",
    "\n",
    "    public var allDifferentiableVariables: AllDifferentiableVariables {\n",
    "        get { return DiffList(u: layers.map({ $0.allDifferentiableVariables })) }\n",
    "        set {\n",
    "            precondition(newValue.u.count == layers.count, \"Differing # of layers: \\(newValue.u.count) and \\(layers.count)\")\n",
    "            for i in 0..<layers.count { layers[i].allDifferentiableVariables = newValue.u[i] }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "\n",
    "extension SequentialLayer: Layer {\n",
    "\n",
    "    public typealias Input = U.Input\n",
    "    public typealias Output = U.Output\n",
    "  \n",
    "    @differentiable(vjp: _appliedDifferentiating(to:in:))\n",
    "    public func applied(to input: Input, in context: Context) -> Output {\n",
    "        var tmp = input\n",
    "        for layer in layers { tmp = layer.applied(to: tmp, in: context) }\n",
    "        return tmp\n",
    "    }\n",
    "   \n",
    "    public func _appliedDifferentiating(to input: Input, in context: Context) -> (\n",
    "        Output, (Output.CotangentVector) -> (CotangentVector, Input.CotangentVector)) {\n",
    "        \n",
    "        var pullbacks: [(U.Output.CotangentVector) -> (U.AllDifferentiableVariables, U.Input.CotangentVector)] = []\n",
    "        var tmp = input\n",
    "        for layer in layers {\n",
    "            let (output, pullback) = Swift.valueWithPullback(at: layer, tmp) { layer, input in\n",
    "                return layer.applied(to: input, in: context)\n",
    "            }\n",
    "            tmp = output\n",
    "            pullbacks.append(pullback)\n",
    "        }\n",
    "        \n",
    "        return (tmp, { input in\n",
    "            var allDiffVars: [U.AllDifferentiableVariables] = []\n",
    "            var tmp = input\n",
    "                    \n",
    "            for pb in pullbacks.reversed() {\n",
    "                let (diffVars, input) = pb(tmp)\n",
    "                tmp = input\n",
    "                allDiffVars.append(diffVars)\n",
    "            }\n",
    "                      \n",
    "            return (DiffList(u: allDiffVars.reversed()), tmp)\n",
    "        })\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MyModel: Layer {\n",
    "    var layers: SequentialLayer<Dense<Float>>\n",
    "    \n",
    "    init(inputSize: Int, hiddenUnits: [Int], outputSize: Int) {\n",
    "        // Make the dense layers.\n",
    "        \n",
    "        // TODO(saeta): Clean up this code.\n",
    "        var input = inputSize\n",
    "        var output = outputSize\n",
    "        if hiddenUnits.count > 0 { output = hiddenUnits[0] }\n",
    "        var layers: [Dense<Float>] = []\n",
    "        for i in 0..<hiddenUnits.count {\n",
    "            output = hiddenUnits[i]\n",
    "            print(\"Making Dense<Float>(inputSize: \\(input), outputSize: \\(output))\")\n",
    "            layers.append(Dense<Float>(inputSize: input, outputSize: output))\n",
    "            input = output\n",
    "        }\n",
    "        print(\"Making Dense<Float>(inputSize: \\(output), outputSize: \\(outputSize))\")\n",
    "        layers.append(Dense<Float>(inputSize: output, outputSize: outputSize))\n",
    "        \n",
    "        self.layers = SequentialLayer(layers: layers)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        return layers.applied(to: input, in: context)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct FixedModel: Layer {\n",
    "    var d1 = Dense<Float>(inputSize: 784, outputSize: 30)\n",
    "    var d2 = Dense<Float>(inputSize: 30, outputSize: 10)\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        return input.sequenced(in: context, through: d1, d2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-15 22:32:24.275221: W tensorflow/core/framework/allocator.cc:122] Allocation of 188160000 exceeds 10% of system memory.\n",
      "2019-04-15 22:32:24.406032: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2019-04-15 22:32:24.440422: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300055000 Hz\n",
      "2019-04-15 22:32:24.570523: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2019-04-15 22:32:24.631664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1009] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-04-15 22:32:24.632543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1559] Found device 0 with properties: \n",
      "name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 7.44GiB freeMemory: 5.53GiB\n",
      "2019-04-15 22:32:24.632577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1682] Adding visible gpu devices: 0\n",
      "2019-04-15 22:32:24.641952: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-04-15 22:32:24.642676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1090] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-04-15 22:32:24.642701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096]      0 \n",
      "2019-04-15 22:32:24.642717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1109] 0:   N \n",
      "2019-04-15 22:32:24.643094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5362 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2)\n"
     ]
    }
   ],
   "source": [
    "var (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath, flat: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func logSumExp<Scalar>(_ x: Tensor<Scalar>) -> Tensor<Scalar> where Scalar:TensorFlowFloatingPoint{\n",
    "    let m = x.max(alongAxes: -1)\n",
    "    return m + log(exp(x-m).sum(alongAxes: -1))\n",
    "}\n",
    "func logSoftmax<Scalar>(_ activations: Tensor<Scalar>) -> Tensor<Scalar> where Scalar:TensorFlowFloatingPoint{\n",
    "    return activations - logSumExp(activations)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trainingContext = Context(learningPhase: .training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let lr:Float = 0.5   // learning rate\n",
    "let epochs = 1      // how many epochs to train for\n",
    "let bs:Int32=64                         // batch size\n",
    "let (n,m) = (60000,784)  // MNIST dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var modelFixed = FixedModel()\n",
    "let modelFixedStart = modelFixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFixedStart.d1.weight == modelFixed.d1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Dense<Float>(inputSize: 784, outputSize: 30)\n",
      "Making Dense<Float>(inputSize: 30, outputSize: 10)\n"
     ]
    }
   ],
   "source": [
    "var modelFlex = MyModel(inputSize: 784, hiddenUnits: [30], outputSize: 10)\n",
    "let modelFlexStart = modelFlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFlex.layers.layers[0].weight == modelFlexStart.layers.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public func accuracy(_ output: Tensor<Float>, _ target: Tensor<Int32>) -> Tensor<Float>{\n",
    "    let corrects = Tensor<Float>(output.argmax(squeezingAxis: 1) .== target)\n",
    "    return corrects.mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let inferenceContext = Context(learningPhase: .inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-15 22:33:19.726267: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1082\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(modelFlex.applied(to: xValid, in: inferenceContext), yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0773\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(modelFixed.applied(to: xValid, in: inferenceContext), yValid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loops below are copied from 03_minibatch. They don't appear to actually train either model. :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1...epochs{\n",
    "    for i in 0..<((n-1)/Int(bs)){\n",
    "        let startIdx = Int32(i) * bs\n",
    "        let endIdx = startIdx + bs\n",
    "        let xb = xTrain[startIdx..<endIdx]\n",
    "        let yb = yTrain[startIdx..<endIdx]\n",
    "        let (loss, grads) = modelFixed.valueWithGradient { model -> Tensor<Float> in\n",
    "            let preds = model.applied(to: xb, in: trainingContext)\n",
    "            return softmaxCrossEntropy(logits: preds, labels: yb)\n",
    "        }\n",
    "        var parameters = modelFixed.allDifferentiableVariables\n",
    "        for kp in parameters.recursivelyAllWritableKeyPaths(to: Tensor<Float>.self){ \n",
    "            parameters[keyPath: kp] -= lr * grads[keyPath:kp]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1...epochs{\n",
    "    for i in 0..<((n-1)/Int(bs)){\n",
    "        let startIdx = Int32(i) * bs\n",
    "        let endIdx = startIdx + bs\n",
    "        let xb = xTrain[startIdx..<endIdx]\n",
    "        let yb = yTrain[startIdx..<endIdx]\n",
    "        let (loss, grads) = modelFlex.valueWithGradient { model -> Tensor<Float> in\n",
    "            let preds = model.applied(to: xb, in: trainingContext)\n",
    "            return softmaxCrossEntropy(logits: preds, labels: yb)\n",
    "        }\n",
    "        var parameters = modelFlex.allDifferentiableVariables\n",
    "        for kp in parameters.recursivelyAllWritableKeyPaths(to: Tensor<Float>.self){ \n",
    "            parameters[keyPath: kp] -= lr * grads[keyPath: kp]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1082\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(modelFlex.applied(to: xValid, in: inferenceContext), yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0773\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(modelFixed.applied(to: xValid, in: inferenceContext), yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFixedStart.d1.weight == modelFixed.d1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFlex.layers.layers[0].weight == modelFlexStart.layers.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let optimizerFixed = SGD<FixedModel, Float>(learningRate: lr)\n",
    "// let optimizerFlex = SGD<MyModel, Float>(learningRate: lr)  // SGD doesn't work for the new flex style models, due to the interaction between how .zero is defined, and keypathing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// Stochastic gradient descent (SGD) optimizer.\n",
    "///\n",
    "/// An optimizer that implements stochastic gradient descent, with support for momentum, learning\n",
    "/// rate decay, and Nesterov momentum.\n",
    "public class SimpleSGD<Model: Layer, Scalar: TensorFlowFloatingPoint>: Optimizer\n",
    "    where Model.AllDifferentiableVariables == Model.CotangentVector {\n",
    "    /// The learning rate.\n",
    "    public var learningRate: Scalar\n",
    "\n",
    "    public init(\n",
    "        learningRate: Scalar = 0.01\n",
    "    ) {\n",
    "        precondition(learningRate >= 0, \"Learning rate must be non-negative\")\n",
    "\n",
    "        self.learningRate = learningRate\n",
    "    }\n",
    "\n",
    "    public func update(_ model: inout Model.AllDifferentiableVariables,\n",
    "                       along direction: Model.CotangentVector) {\n",
    "        for kp in model.recursivelyAllWritableKeyPaths(to: Tensor<Scalar>.self) {\n",
    "            model[keyPath: kp] -= -learningRate * direction[keyPath: kp]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let simpleOptFlex = SimpleSGD<MyModel, Float>(learningRate: lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1...epochs{\n",
    "    for i in 0..<((n-1)/Int(bs)){\n",
    "        let startIdx = Int32(i) * bs\n",
    "        let endIdx = startIdx + bs\n",
    "        let xb = xTrain[startIdx..<endIdx]\n",
    "        let yb = yTrain[startIdx..<endIdx]\n",
    "        let (loss, grads) = modelFixed.valueWithGradient { model -> Tensor<Float> in\n",
    "            let preds = model.applied(to: xb, in: trainingContext)\n",
    "            return softmaxCrossEntropy(logits: preds, labels: yb)\n",
    "        }\n",
    "        optimizerFixed.update(&modelFixed.allDifferentiableVariables, along: grads)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.098\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(modelFixed.applied(to: xValid, in: inferenceContext), yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFixedStart.d1.weight == modelFixed.d1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1...epochs{\n",
    "    for i in 0..<((n-1)/Int(bs)){\n",
    "        let startIdx = Int32(i) * bs\n",
    "        let endIdx = startIdx + bs\n",
    "        let xb = xTrain[startIdx..<endIdx]\n",
    "        let yb = yTrain[startIdx..<endIdx]\n",
    "        let (loss, grads) = modelFlex.valueWithGradient { model -> Tensor<Float> in\n",
    "            let preds = model.applied(to: xb, in: trainingContext)\n",
    "            return softmaxCrossEntropy(logits: preds, labels: yb)\n",
    "        }\n",
    "//        optimizerFlex.update(&modelFlex.allDifferentiableVariables, along: grads)\n",
    "        simpleOptFlex.update(&modelFlex.allDifferentiableVariables, along: grads)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.098\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(modelFlex.applied(to: xValid, in: inferenceContext), yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFlex.layers.layers[0].weight == modelFlexStart.layers.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
